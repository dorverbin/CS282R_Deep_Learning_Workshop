{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entirety of section is taken from [this](http://www.anishathalye.com/2017/07/25/synthesizing-adversarial-examples/) tutorial on adversarial examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesizing [adversarial examples](https://arxiv.org/abs/1312.6199) for neural networks is surprisingly easy: small, carefully-crafted perturbations to inputs can cause neural networks to misclassify inputs in arbitrarily chosen ways. Given that adversarial examples [transfer to the physical world](https://arxiv.org/abs/1607.02533) and [can be made extremely robust](https://blog.openai.com/robust-adversarial-inputs/), this is a real security concern.\n",
    "\n",
    "In this post, we give a brief introduction to algorithms for synthesizing adversarial examples, and we walk through the process of implementing attacks in [TensorFlow](https://www.tensorflow.org/), building up to synthesizing a robust adversarial example following [this technique](https://arxiv.org/abs/1707.07397).\n",
    "\n",
    "**This post is an executable [Jupyter notebook](http://jupyter.org/): you're encouraged to [download it](/media/2017/07/25/adversarial.ipynb) and experiment with the examples yourself!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup\n",
    "\n",
    "We choose to attack an [Inception v3](https://arxiv.org/abs/1512.00567) network trained on [ImageNet](http://www.image-net.org/). In this section, we load a pre-trained network from the [TF-slim image classification library](https://github.com/tensorflow/models/tree/master/slim). This part isn't particularly interesting, so **feel free to [skip this section](#adversarial-examples)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets as nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the input image. We use a `tf.Variable` instead of a `tf.placeholder` because we will need it to be trainable. We can still feed it when we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the Inception v3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(image, reuse):\n",
    "    preprocessed = tf.multiply(tf.subtract(tf.expand_dims(image, 0), 0.5), 2.0)\n",
    "    arg_scope = nets.inception.inception_v3_arg_scope(weight_decay=0.0)\n",
    "    with slim.arg_scope(arg_scope):\n",
    "        logits, _ = nets.inception.inception_v3(\n",
    "            preprocessed, 1001, is_training=False, reuse=reuse)\n",
    "        logits = logits[:,1:] # ignore background class\n",
    "        probs = tf.nn.softmax(logits) # probabilities\n",
    "    return logits, probs\n",
    "\n",
    "logits, probs = inception(image, reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load pre-trained weights. This Inception v3 has a top-5 accuracy of 93.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = tempfile.mkdtemp()\n",
    "inception_tarball, _ = urlretrieve(\n",
    "    'http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz')\n",
    "tarfile.open(inception_tarball, 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_vars = [\n",
    "    var for var in tf.global_variables()\n",
    "    if var.name.startswith('InceptionV3/')\n",
    "]\n",
    "saver = tf.train.Saver(restore_vars)\n",
    "saver.restore(sess, os.path.join(data_dir, 'inception_v3.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write some code to show an image, classify it, and show the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_json, _ = urlretrieve(\n",
    "    'http://www.anishathalye.com/media/2017/07/25/imagenet.json')\n",
    "with open(imagenet_json) as f:\n",
    "    imagenet_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(img, correct_class=None, target_class=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "    fig.sca(ax1)\n",
    "    p = sess.run(probs, feed_dict={image: img})[0]\n",
    "    ax1.imshow(img)\n",
    "    fig.sca(ax1)\n",
    "    \n",
    "    topk = list(p.argsort()[-10:][::-1])\n",
    "    topprobs = p[topk]\n",
    "    barlist = ax2.bar(range(10), topprobs)\n",
    "    if target_class in topk:\n",
    "        barlist[topk.index(target_class)].set_color('r')\n",
    "    if correct_class in topk:\n",
    "        barlist[topk.index(correct_class)].set_color('g')\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(10),\n",
    "               [imagenet_labels[i][:15] for i in topk],\n",
    "               rotation='vertical')\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image\n",
    "\n",
    "We load our example image and make sure it's classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, _ = urlretrieve('http://www.anishathalye.com/media/2017/07/25/cat.jpg')\n",
    "img_class = 281\n",
    "img = PIL.Image.open(img_path)\n",
    "big_dim = max(img.width, img.height)\n",
    "wide = img.width > img.height\n",
    "new_w = 299 if not wide else int(img.width * 299 / img.height)\n",
    "new_h = 299 if wide else int(img.height * 299 / img.width)\n",
    "img = img.resize((new_w, new_h)).crop((0, 0, 299, 299))\n",
    "img = (np.asarray(img) / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(img, correct_class=img_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial examples\n",
    "\n",
    "Given an image $\\mathbf{x}$, our neural network outputs a probability distribution over labels, $P(y \\mid \\mathbf{x})$. When we craft an adversarial input, we want to find an $\\hat{\\mathbf{x}}$ where $\\log P(\\hat{y} \\mid \\hat{\\mathbf{x}})$ is maximized for a target label $\\hat{y}$: that way, our input will be misclassified as the target class. We can ensure that $\\hat{\\mathbf{x}}$ doesn't look too different from the original $\\mathbf{x}$ by constraining ourselves to some $\\ell_\\infty$ box with radius $\\epsilon$, requiring that $\\left\\lVert \\mathbf{x} - \\hat{\\mathbf{x}} \\right\\rVert_\\infty \\le \\epsilon$.\n",
    "\n",
    "In this framework, an adversarial example is the solution to a constrained optimization problem that we can solve using [backpropagation](http://colah.github.io/posts/2015-08-Backprop/) and projected gradient descent, basically the same techniques that are used to train networks themselves. The algorithm is simple:\n",
    "\n",
    "We begin by initializing our adversarial example as $\\hat{\\mathbf{x}} \\leftarrow \\mathbf{x}$. Then, we repeat the following until convergence:\n",
    "\n",
    "1. $\\hat{\\mathbf{x}} \\leftarrow \\hat{\\mathbf{x}} + \\alpha \\cdot \\nabla \\log P(\\hat{y} \\mid \\hat{\\mathbf{x}})$\n",
    "2. $\\hat{\\mathbf{x}} \\leftarrow \\mathrm{clip}(\\hat{\\mathbf{x}}, \\mathbf{x} - \\epsilon, \\mathbf{x} + \\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "We start with the easiest part: writing a TensorFlow op for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (299, 299, 3))\n",
    "\n",
    "x_hat = image # our trainable adversarial input\n",
    "assign_op = tf.assign(x_hat, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent step\n",
    "\n",
    "Next, we write the gradient descent step to maximize the log probability of the target class (or equivalently, minimize the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32, ())\n",
    "y_hat = tf.placeholder(tf.int32, ())  # Target label\n",
    "labels = tf.one_hot(y_hat, 1000)\n",
    "\n",
    "# Exercise 1: implement step 1.\n",
    "# Define an optimizer object similar to what we saw last week.\n",
    "# Then, define a optimization step object optimizer.minimize(),\n",
    "# noting that we do not need to optimize any weight inside\n",
    "# the network, but only the adversarial image x_hat. This can\n",
    "# be done by passing a var_list argument to the minimize() function.\n",
    "# Hint: in order to define the loss function of the network, you might\n",
    "# want to use tf.nn.softmax_cross_entropy_with_logits().\n",
    "\n",
    "### Insert code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection step\n",
    "\n",
    "Finally, we write the projection step to keep our adversarial example visually close to the original image. Additionally, we clip to $[0, 1]$ to keep it a valid image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = tf.placeholder(tf.float32, ())\n",
    "\n",
    "# Exercise 2: implement step 2.\n",
    "# You might want to use tf.clip_by_value() and tf.assign()\n",
    "# to set the result of the clipping operation to x_hat.\n",
    "# Also, remember that a valid image has values in [0, 1].\n",
    "\n",
    "### Insert code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Finally, we're ready to synthesize an adversarial example. We arbitrarily choose \"guacamole\" (imagenet class 924) as our target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_epsilon = 2.0/255.0 # a really small perturbation\n",
    "demo_lr = 1e-1\n",
    "demo_steps = 100\n",
    "demo_target = 924 # \"guacamole\"\n",
    "\n",
    "# initialization step\n",
    "sess.run(assign_op, feed_dict={x: img})\n",
    "\n",
    "# projected gradient descent\n",
    "for i in range(demo_steps):\n",
    "    # gradient descent step\n",
    "    _, loss_value = sess.run(\n",
    "        [optim_step, loss],\n",
    "        feed_dict={learning_rate: demo_lr, y_hat: demo_target})\n",
    "    # project step\n",
    "    sess.run(project_step, feed_dict={x: img, epsilon: demo_epsilon})\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('step %d, loss=%g' % (i+1, loss_value))\n",
    "    \n",
    "\n",
    "adv = x_hat.eval() # retrieve the adversarial example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adversarial image is visually indistinguishable from the original, with no visual artifacts. However, it's classified as \"guacamole\" with high probability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(adv, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
